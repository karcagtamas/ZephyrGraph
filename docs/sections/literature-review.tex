\chapter{Literature Review}
\label{ch:literature-review}

\section{Cause-Effect Graphs in Software Testing}

Cause-effect graphs are a well-established technique in software testing \cite{ce-technique}, used to model and visualize the logical dependencies between various inputs and outputs in a target system. They are frequently employed alongside black-box testing automation techniques such as boundary value analysis, equivalence class partitioning, and pairwise testing. Originally developed in the 1970s for testing hardware logic circuits, this black-box technique has undergone significant changes over time, particularly in terms of its notation. Elements such as nodes, logical relationships, and constraints have been improved, making it an essential tool for detecting faults in business logic. Their structured nature allows testers to systematically identify combinations of input conditions that lead to specific outputs. The graph visually represents the logical connections between inputs and outputs, which can be formulated as Boolean expressions.

Also referred to as dependency modeling, it emphasizes capturing the relationships between input conditions and their corresponding effects in a program \cite{ceg}.

\subsection{Role of Cause-Effect Graphs in Testing}

In software testing, cause-effect graphs are typically used to generate test cases by mapping relationships between inputs and outputs \cite{myers2011art}. Each "cause" represents an input or condition, while each "effect" corresponds to an output or action that the system performs based on the input conditions. The graphical representation helps testers analyze how changes in input conditions propagate through the system and affect the output. This makes cause-effect graphs particularly useful for identifying boundary conditions, invalid input combinations, and hidden dependencies that might not be immediately obvious in more traditional forms of testing like unit testing or manual exploratory testing.

\subsection{Advantages of Cause-Effect Graphs}

\begin{enumerate}
	\item \textbf{Clarity and Structure}: Cause-effect graphs provide a clear, visual way to represent complex logical relationships. The business specifications are divided into workable pieces \cite{myers2011art}. This makes it easier for testers to see the full range of potential interactions between causes and effects. Additionally, all causes and effects are clearly identified, helping to avoid any potential confusion.
	\item \textbf{Test Case Generation}: The structured nature of cause-effect graphs allows for systematic test case generation \cite{myers2011art}. By identifying all possible combinations of input conditions, testers can generate a set of comprehensive test cases that aim to cover every logical path within the system.
	\item \textbf{Error Detection}: Because cause-effect graphs enforce a methodical approach to analyzing inputs and outputs, they help identify logical errors that may not be immediately evident, such as missing conditions, invalid input combinations, or contradictory business rules.
\end{enumerate}

\subsection{Challenges and Limitations}

While cause-effect graphs offer several benefits, they also present certain challenges when applied to modern, complex software systems. One of the main issues is scalability. As the complexity of an application grows, so does the number of possible input conditions and their interdependencies. In such cases, cause-effect graphs can become unwieldy, making it difficult to manage the relationships and generate meaningful test cases.

Additionally, manual intervention is often required to transition from cause-effect graphs to actual test cases. This process can be time-consuming and prone to human error, particularly when dealing with highly complex systems with many variables. Current methods for automating test generation from cause-effect graphs remain limited in their ability to handle scalability and optimization, especially in large, evolving software systems.

Moreover, this graphical solution can be interpreted differently across various enterprises, and managing the graphs may become time-consuming and difficult, depending on the editor interface used.

\subsection{Applications of Cause-Effect Graphs in Industry}

Despite these challenges, cause-effect graphs have been successfully applied in various industries, particularly in safety-critical systems \cite{ce-technique}, where comprehensive testing is essential to ensure reliability and correctness. For example, they are commonly used in:

\begin{itemize}
	\item \textbf{Embedded systems}: Where complex interactions between hardware and software must be tested.
	\item \textbf{Aerospace and automotive industries}: For validating control systems that depend on numerous inputs.
	\item \textbf{Financial applications}: Where regulatory compliance and error-free operation are critical.
\end{itemize}

These examples illustrate how cause-effect graphs help ensure that all potential scenarios are considered and tested, thereby minimizing the risk of system failures or regulatory non-compliance.

\subsection{Research Gaps and Opportunities}

Although cause-effect graphs have been widely used in software testing, there are gaps in automating the test case generation process. The transition from graph representation to executable test cases often requires manual effort, and existing approaches struggle with scalability when applied to large, evolving systems. There is a need for more advanced methodologies when a software system is growning in complexity. That can automatically convert cause-effect graphs into optimized test cases, reducing the the manual processes and improving the overall efficiency of testing.

\section{Automated Test Generation}

Automated test generation is a key area in software testing that aims to reduce the time, effort, and human error involved in manually creating test cases. As modern systems grow in complexity, traditional manual testing approaches struggle to keep pace with the increasing number of possible input combinations and interactions. Automated test generation addresses this challenge by leveraging algorithms and tools to automatically create test cases based on the system's structure, behavior, or specification \cite{auto-test}.

\subsection{Importance of Automated Test Generation}

Automated test generation plays a crucial role in enhancing testing efficiency and ensuring thorough test coverage of the subject under test. Achieving high and accurate coverage helps ensure the correctness of the target. Therefore, the automation process must account for all relevant paths, conditions, and input combinations to achieve the desired level of quality.

Identifying defects early in the software development lifecycle is a crucial step in the development process. Through automation, testers can concentrate on analyzing results and optimizing the test environment, rather than wasting time on the repetitive tasks of manually defining test cases or searching for test data. It also minimizes the risk of human error \cite{auto-test-gen}.

Moreover, automation provides repeatable, reliable test scenarios that can be executed quickly. This allowing for continuous integration and frequent testing in agile development environments. This capability improves the chances of identifying edge cases and scenarios that manual methods might miss, and it can occur in the early stages of development, reducing the cost of the fixing process.

\subsection{Approaches to Automated Test Generation}

There are several approaches to automated test generation, each with its own strengths and weaknesses. I briefly present a few approaches, though there are many others available.

\begin{itemize}
	\item \textbf{Random Test Generation}: Creates test cases by selecting inputs randomly. Easy to implement, useful in stress testing but often result in poor test coverage and may miss critical paths \cite{auto-test-gen}.
	\item \textbf{Model-Based Testing (MBT)}: Uses model of the system's behavior, such as finite state machines, decision tables, or UML diagrams, to generate test cases. MBT provides a high level of abstraction and the testers can fouc on logic and behavior. It can be a highly effective approach, but it mainly depends on the quality and accuracy of the model \cite{utting2010practical}. In the case of Model-Based Testing (MBT), the test case generation process is based on specifications, without any knowledge of the internal code structure.
	\item \textbf{Code-Based Test Generation}: Focuses on the internal structure of the code base. Techniques such as control flow analysis, data flow analysis, and symbolic execution are used to generate test cases. Code coverage metrics, such as statement, branch, or path coverage, guide the generation process to ensure all parts of the code are tested. This method is effective in identifying issues related to internal logic but requires access to the codebase \cite{clarke1976system, gupta2000generating}.
	\item \textbf{Constraint-Based Test Generation}: This approach defines the testing problem through a set of constraints that represent the conditions under which specific behaviors should occur. Solvers are then employed to generate test cases that meet these constraints. Constraint-based generation is especially effective for testing systems with complex business rules, as it ensures that the resulting test cases are both meaningful and relevant.
\end{itemize}

\subsection{Tools for Automated Test Generation}

Several tools and frameworks have been developed to support automated test generation across different domains. Some popular tools include:

\begin{itemize}
	\item \textbf{QuickCheck}: QuickCheck is a software library, originally written in the programming language Haskell, designed to assist in software testing by generating test cases for test suites \cite{claessen2000quickcheck}.
	\item \textbf{JUnit + EvoSuite}: EvoSuite is a tool that automatically generates test cases with assertions for classes written in Java code \cite{fraser2013evosuite}.
	\item \textbf{TTCN-3}: The Testing and Test Control Notation Version 3 (TTCN-3) is a standardized testing technology, specifically designed for testing and certification. It is primarily used in telecommunications systems \cite{serbanescu2008real}.
	\item \textbf{Pex}: Pex can automatically generate test suites with high code coverage through automated white-box analysis and is designed for testing .NET applications \cite{tillmann2008pex}.
\end{itemize}

I won't go into detail on each of these tools individually, as they use different algorithms and environments to achieve the same or similar objectives.

\subsection{Challenges in Automated Test Generation}

Despite its advantages, automated test generation also faces several challenges and there some of them:

\subsubsection{Scalability}

As software systems grow in size and complexity, the number of possible input combinations and execution paths increases exponentially. Automated tools may struggle to handle the sheer volume of potential test cases, leading to issues with performance and efficiency \cite{candea2019automated}.

\subsubsection{Test Case Optimization}

Generating a large number of test cases is not always practical, as it may result in redundant or irrelevant tests. Optimizing the test set to cover the most critical paths and conditions without unnecessary duplication remains a challenge in automated test generation.

\subsubsection{Handling Evolving Systems}

In dynamic or continuously evolving systems, automated test generation must adapt to frequent changes in the codebase or system behavior. Ensuring that the generated test cases remain relevant and effective as the system evolves is a key challenge.

\subsubsection{Integration with Existing Systems}

Many organizations already have established testing processes and tools. Integrating automated test generation with these legacy systems can be difficult task, particularly if the automated tools produce test cases that do not align with existing formats or methodologies.

\subsubsection{Environment}

To test real-world programs, a symbolic execution engine must mediate between the program and its runtime environment, such as external libraries, the operating system, thread and process schedulers, I/O interrupt events, and other environment-dependent components. As a result, symbolic execution engines must minimize the time spent interacting with the environment while ensuring the program's correct behavior \cite{candea2019automated}.

\subsection{Research Gaps and Future Directions}

Although automated test generation has made significant progress, there are still areas that require further research. One such area is improving the scalability and optimization of test generation tools to handle complex and large-scale systems. Additionally, more sophisticated test oracles are needed to automate the validation of test results.

Another promising direction is the integration of artificial intelligence (AI) and machine learning (ML) techniques into test generation \cite{khan2024new}. These technologies have the potential to enhance test generation tools by predicting failure-prone areas of the system, identifying critical test cases, and adapting to evolving software systems.

\section{Logical Systems and Testing}

Logical systems form the backbone of many software applications, particularly those involving complex decision-making processes, business rules, or conditional logic. These systems are typically composed of a set of logical rules, conditions, and actions that define how inputs are processed and how outputs are generated. In the context of software testing, ensuring the correctness and reliability of such logical systems is critical, as defects in the underlying logic can lead to incorrect outputs, business failures, or security vulnerabilities \cite{logic}.

\subsection{Role of Logic in Software Testing}

Testing logical systems often involves verifying that the system's behavior aligns with the expected logical outcomes under all possible input combinations. This includes ensuring that all business rules, conditions, and decision paths are covered, and that the system behaves correctly in both typical and edge cases \cite{myers2011art}. Logical systems can vary in complexity, from simple decision trees to intricate rule-based engines that handle numerous variables and interdependencies.

Testing these systems presents unique challenges, particularly when the number of conditions grows large. Logical systems testing requires techniques that can efficiently explore all possible outcomes and identify inconsistencies, contradictions, or gaps in the logic.

\subsection{Logical Formulas and Testing}

Logical systems of an application are often represented using formal logic, such as propositional logic or first-order logic, to describe the relationships between inputs and outputs. In software testing, these logical formulas are used to create test cases, ensuring that the system responds as expected under every scenario.

One key aspect of testing logical systems is converting the business logic into a formal representation that can be evaluated and tested systematically. This formal representation typically takes the form of logical expressions, which can then be analyzed to generate test cases. For example, in propositional logic, inputs and outputs can be represented as Boolean variables, and logical operators (AND, OR, NOT) can define the relationships between them. These formulas can be evaluated to check whether the system's logic holds true under various conditions.

\subsection{Disjunctive Normal Form (DNF)}

Disjunctive Normal Form (DNF) is a logical representation used in various fields, including software testing, to simplify and analyze logical conditions. In DNF, a logical formula is expressed as a disjunction (OR) of multiple conjunctions (AND) of literals, where each conjunction represents a combination of conditions that must be true for the overall expression to evaluate as true \cite{normal-forms}.

In software testing, DNF can be particularly useful when testing complex conditional logic or decision-making structures within a program. By breaking down the logical conditions into a DNF format, testers can more easily identify the different possible scenarios or paths that need to be tested. Each conjunction in the DNF represents a unique combination of conditions that can lead to a certain outcome, helping to ensure that all relevant cases, including edge cases, are covered.

DNF is often employed in model-based testing, constraint-based testing, or formal verification, where precise logical expressions are needed to define input-output relationships or system behavior. By structuring conditions in DNF, testers can systematically generate test cases, improving coverage and ensuring that the software behaves as expected across a wide range of inputs.

\subsection{Decision Tables}

A decision table is a systematic way to represent and analyze complex decision-making logic in software testing. It helps to organize and capture various combinations of inputs and the corresponding actions or outcomes in a tabular format. This approach is especially useful when testing systems with multiple conditions or rules, ensuring that all possible scenarios are covered and reducing the risk of overlooking important cases \cite{normal-forms}.

Decision tables provide a structured and efficient way to handle logical complexity in testing, making them an essential tool for achieving high test coverage in condition-based testing scenarios.

\subsubsection{Components of a Decision Table}

\begin{compactitem}
	\item \textbf{Conditions}: These represent the different input variables or factors that affect the decision.
	\item \textbf{Actions}: These define the expected outcomes or actions the system should take based on the input conditions.
	\item \textbf{Rules}: Each row in the decision table, known as a rule, represents a unique combination of conditions and the corresponding action.
\end{compactitem}

\subsubsection{Example of a Decision Table}

Let us see an example of a decision table for testing a login system with conditions for user authentication. We want to test the login functionality of a website. The system checks the following conditions to determine whether to allow access or display an error message.

\textbf{Conditions}:

\begin{compactenum}
	\item Username is correct (\textbf{C1})
	\item Password is correct (\textbf{C2})
	\item Two-factor authentication enabled (\textbf{C3})
\end{compactenum}

\textbf{Actions}:

\begin{compactenum}
	\item Grant access (\textbf{A1})
	\item Display "Invalid username/password" error (\textbf{A2})
	\item Prompt for two-factor authentication (\textbf{A3})
\end{compactenum}

\begin{table}[H]
	\centering
	\begin{tabular}{ | m{0.08\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | }
		\hline
		\textbf{Rules} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{A1} & \textbf{A2} & \textbf{A3} \\
		\hline \hline
		1 & Yes & Yes & No & Yes & No & No \\
		\hline
		2 & Yes & Yes & Yes & No & No & Yes \\
		\hline
		3 & Yes & No & No & No & Yes & No \\
		\hline
		4 & Yes & No & Yes & No & Yes & No \\
		\hline
		5 & No & Any & Any & No & Yes & No \\
		\hline
	\end{tabular}
	\caption{Decision table authentication example}
	\label{tab:dec-table-example}
\end{table}

\subsubsection{Advantages of Decision Tables}:

\begin{itemize}
	\item \textbf{Comprehensive Coverage}: They help ensure that all possible combinations of conditions and actions are considered.
	\item \textbf{Clarity}: The tabular format makes it easy to visualize complex decision logic.
	\item \textbf{Easy to Automate}: Decision tables are well-suited for automated test case generation.
	\item \textbf{Reduces Redundancy}: By clearly defining rules, they help avoid redundant or unnecessary tests.
\end{itemize}

\subsection{Challenges in Testing Logical Systems}

Testing logical systems poses several challenges, particularly in terms of scalability and complexity. As we mentioned previously, the number of conditions, variables, and interdependencies increases, the number of possible input combinations grows exponentially, making it difficult to manually generate test cases that cover all possible scenarios. Automation is essential for testing logical rule sets, as manual testing can be time-consuming and prone to human error.

Another challenge is ensuring that the system's logical representation accurately reflects the intended business rules. Even minor errors in the formulation of logical rules can lead to significant issues in the system's behavior. Validating the logical rule sets is a crucial step to ensure the correctness of the generated test suites and the accuracy of the pre-deployment testing phase.

\section{Summary of Research Gaps}

While there has been significant progress in automated software testing and the use of logical systems such as cause-effect graphs, several research gaps remain that hinder full optimization and effectiveness in real-world applications \cite{ce-technique, ceg}:

\begin{enumerate}
	\item \textbf{Scalability of Cause-Effect Graph Testing}: Current methods for generating test cases from cause-effect graphs struggle with scalability, especially in systems with large numbers of conditions or rapidly evolving business rules. As the complexity of logical systems increases, the manual or semi-automated approaches often become inefficient, leading to incomplete test coverage.
	\item \textbf{Transition from Cause-Effect Graphs to Test Cases}: Although cause-effect graphs are useful for modeling business logic, converting these graphical representations into meaningful, actionable test cases is still a challenge. Many current approaches rely on manual intervention or specialized tools that are difficult to generalize or integrate with other systems.
	\item \textbf{Constraint Optimization in Test Generation}: While constraint-based test generation is effective for systems with complex business rules, optimizing the constraint-solving process to handle larger, more dynamic systems is still an area requiring improvement. The existing techniques often focus on static systems, and there is limited research on optimizing test generation for systems that continuously evolve.
	\item \textbf{Tool Support and Integration}: Although many tools exist for automated test generation, they often use different algorithms and environments, leading to fragmentation. The lack of standardized methods and interoperable tools makes it difficult for organizations to adopt and scale these solutions effectively. Additionally, the handling and management of cause-effect graphs vary widely, making it time-consuming and challenging depending on the tool or editor used.
	\item \textbf{Handling Dynamic and Evolving Systems}: As software systems evolve over time, the ability to adapt test cases to changing business logic remains underdeveloped. Current research has not fully addressed how to dynamically update test cases when logical rules change, leading to gaps in coverage and potential defects in evolving applications.
	\item \textbf{Validation and Accuracy of Generated Test Suites}: Ensuring the accuracy and correctness of automatically generated test suites remains a critical challenge. While automation has reduced the risk of human error, more robust methods are needed to validate the logical consistency of test suites, particularly in pre-deployment testing phases.
\end{enumerate}

Addressing these research gaps could lead to more efficient, scalable, and accurate approaches to automated test generation, especially for complex business logic and dynamic systems. Further research is needed to enhance scalability, tool integration, and the adaptation of testing methods for evolving systems.